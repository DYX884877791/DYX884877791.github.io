(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{228:function(a,t,r){a.exports=r.p+"assets/img/kafka.dd3dee36.png"},229:function(a,t,r){a.exports=r.p+"assets/img/partition.be43f5c7.png"},230:function(a,t,r){a.exports=r.p+"assets/img/partition1.dd381391.png"},231:function(a,t,r){a.exports=r.p+"assets/img/kafka_zk.2d718546.png"},232:function(a,t,r){a.exports=r.p+"assets/img/broker1.4cef0f31.png"},233:function(a,t,r){a.exports=r.p+"assets/img/kafka_write.0cfcbd88.jpg"},248:function(a,t,r){"use strict";r.r(t);var e=r(0),s=Object(e.a)({},function(){var a=this,t=a.$createElement,e=a._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"kafka"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#kafka","aria-hidden":"true"}},[a._v("#")]),a._v(" Kafka")]),a._v(" "),e("h2",{attrs:{id:"kafka简介"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#kafka简介","aria-hidden":"true"}},[a._v("#")]),a._v(" Kafka简介")]),a._v(" "),e("p",[a._v("Kafka是一个"),e("strong",[a._v("分布式")]),a._v("的"),e("strong",[a._v("基于发布/订阅模式")]),a._v("的"),e("strong",[a._v("消息队列")]),a._v("，主要应用于大数据实时处理领域")]),a._v(" "),e("h3",{attrs:{id:"消息队列"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#消息队列","aria-hidden":"true"}},[a._v("#")]),a._v(" 消息队列")]),a._v(" "),e("p",[a._v("消息队列中间件是分布式系统中重要的组件，主要解决应用解耦，异步消息，流量削锋等问题，实现高性能，高可用，可伸缩和最终一致性架构。目前使用较多的消息队列有ActiveMQ，RabbitMQ，ZeroMQ，Kafka，MetaMQ，RocketMQ。\n消息队列的两种模式：")]),a._v(" "),e("ol",[e("li",[e("p",[a._v("点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）\n点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。")])]),a._v(" "),e("li",[e("p",[a._v("发布/订阅模式（一对多，数据生产后，推送给所有订阅者）\n发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，处于离线状态。")])])]),a._v(" "),e("h3",{attrs:{id:"为什么需要消息队列"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#为什么需要消息队列","aria-hidden":"true"}},[a._v("#")]),a._v(" 为什么需要消息队列")]),a._v(" "),e("ol",[e("li",[e("p",[a._v("解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。")])]),a._v(" "),e("li",[e("p",[a._v('冗余：消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的"插入-获取-删除"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。')])]),a._v(" "),e("li",[e("p",[a._v("扩展性：因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。")])]),a._v(" "),e("li",[e("p",[a._v("灵活性 & 峰值处理能力：在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。")])]),a._v(" "),e("li",[e("p",[a._v("可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。")])]),a._v(" "),e("li",[e("p",[a._v("顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性）")])]),a._v(" "),e("li",[e("p",[a._v("缓冲：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。")])]),a._v(" "),e("li",[e("p",[a._v("异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。")])])]),a._v(" "),e("h3",{attrs:{id:"kafka-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#kafka-2","aria-hidden":"true"}},[a._v("#")]),a._v(" Kafka")]),a._v(" "),e("ol",[e("li",[a._v("Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。")]),a._v(" "),e("li",[a._v("Kafka最初是由LinkedIn公司开发，并于2011年初开源。2012年10月从Apache Incubator毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。")]),a._v(" "),e("li",[a._v("Kafka是一个分布式消息队列。Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer，消息接受者称为Consumer，此外kafka集群有多个kafka实例组成，每个实例(server)称为broker。")]),a._v(" "),e("li",[a._v("无论是kafka集群，还是consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性。")])]),a._v(" "),e("h3",{attrs:{id:"kafka架构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#kafka架构","aria-hidden":"true"}},[a._v("#")]),a._v(" Kafka架构")]),a._v(" "),e("p",[e("a",{attrs:{"data-fancybox":"",title:"",href:"/assets/img/kafka.dd3dee36.png"}},[e("img",{attrs:{src:r(228),alt:""}})])]),a._v(" "),e("ol",[e("li",[e("p",[a._v("Producer ：消息生产者，就是向kafka broker发消息的客户端；")])]),a._v(" "),e("li",[e("p",[a._v("Consumer ：消息消费者，向kafka broker取消息的客户端；")])]),a._v(" "),e("li",[e("p",[a._v("Topic ：可以理解为一个队列；")])]),a._v(" "),e("li",[e("p",[a._v("Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个partion只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic；")])]),a._v(" "),e("li",[e("p",[a._v("Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic；")])]),a._v(" "),e("li",[e("p",[a._v("Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序；")])]),a._v(" "),e("li",[e("p",[a._v("Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka；")])])]),a._v(" "),e("h2",{attrs:{id:"集群部署"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#集群部署","aria-hidden":"true"}},[a._v("#")]),a._v(" 集群部署")]),a._v(" "),e("ol",{attrs:{start:"0"}},[e("li",[e("p",[a._v("准备三台机器：hadoop02、hadoop03、hadoop04")])]),a._v(" "),e("li",[e("p",[a._v("使用kafka_2.11-0.11.0.2.tgz包，并解压")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 module"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ "),e("span",{pre:!0,attrs:{class:"token function"}},[a._v("cd")]),a._v(" kafka_2.11-0.11.0.2/\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 kafka_2.11-0.11.0.2"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ll\n总用量 56\ndrwxr-xr-x.  3 dyx dyx  4096 11月 11 2017 bin\ndrwxr-xr-x.  2 dyx dyx  4096 4月  28 00:31 config\ndrwxr-xr-x.  2 dyx dyx  4096 4月  22 06:50 libs\n-rw-r--r--.  1 dyx dyx 28824 11月 11 2017 LICENSE\ndrwxrwxr-x. 20 dyx dyx  4096 4月  22 09:14 logs\n-rw-r--r--.  1 dyx dyx   336 11月 11 2017 NOTICE\ndrwxr-xr-x.  2 dyx dyx  4096 11月 11 2017 site-docs\n")])])])]),a._v(" "),e("li",[e("p",[a._v("编辑config/server.properties")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 指定brokerid，不同机器节点的broker.id必须不同")]),a._v("\nbroker.id"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("0 \n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 指定可以删除topic，默认为false")]),a._v("\ndelete.topic.enable"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("true\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 处理网络请求的线程数量")]),a._v("\nnum.network.threads"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("3\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 用来处理磁盘IO的现成数量")]),a._v("\nnum.io.threads"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("8\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 发送套接字的缓冲区大小")]),a._v("\nsocket.send.buffer.bytes"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("102400\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 接收套接字的缓冲区大小")]),a._v("\nsocket.receive.buffer.bytes"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("102400\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 请求套接字的缓冲区大小")]),a._v("\nsocket.request.max.bytes"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("104857600\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# kafka运行日志存放的路径，这个目录不止存放日志，kafka存储的数据也会放在此目录，需要预先创建\t")]),a._v("\nlog.dirs"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("/opt/module/kafka/logs\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# topic在当前broker上的分区个数")]),a._v("\nnum.partitions"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("1\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 用来恢复和清理data下数据的线程数量")]),a._v("\nnum.recovery.threads.per.data.dir"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("1\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# kafka存储的数据保留的最长时间，超时将被删除，默认为7天")]),a._v("\nlog.retention.hours"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("168\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 配置连接Zookeeper的集群地址")]),a._v("\nzookeeper.connect"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("hadoop02:2181,hadoop03:2181,hadoop04:2181\n")])])])]),a._v(" "),e("li",[e("p",[a._v("bin目录下的脚本文件")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ll\n总用量 124\n-rwxr-xr-x. 1 dyx dyx 1335 11月 11 2017 connect-distributed.sh\n-rwxr-xr-x. 1 dyx dyx 1332 11月 11 2017 connect-standalone.sh\n-rwxr-xr-x. 1 dyx dyx  861 11月 11 2017 kafka-acls.sh\n-rwxr-xr-x. 1 dyx dyx  873 11月 11 2017 kafka-broker-api-versions.sh\n-rwxr-xr-x. 1 dyx dyx  864 11月 11 2017 kafka-configs.sh\n-rwxr-xr-x. 1 dyx dyx  945 11月 11 2017 kafka-console-consumer.sh\n-rwxr-xr-x. 1 dyx dyx  944 11月 11 2017 kafka-console-producer.sh\n-rwxr-xr-x. 1 dyx dyx  871 11月 11 2017 kafka-consumer-groups.sh\n-rwxr-xr-x. 1 dyx dyx  872 11月 11 2017 kafka-consumer-offset-checker.sh\n-rwxr-xr-x. 1 dyx dyx  948 11月 11 2017 kafka-consumer-perf-test.sh\n-rwxr-xr-x. 1 dyx dyx  869 11月 11 2017 kafka-delete-records.sh\n-rwxr-xr-x. 1 dyx dyx  862 11月 11 2017 kafka-mirror-maker.sh\n-rwxr-xr-x. 1 dyx dyx  886 11月 11 2017 kafka-preferred-replica-election.sh\n-rwxr-xr-x. 1 dyx dyx  959 11月 11 2017 kafka-producer-perf-test.sh\n-rwxr-xr-x. 1 dyx dyx  874 11月 11 2017 kafka-reassign-partitions.sh\n-rwxr-xr-x. 1 dyx dyx  868 11月 11 2017 kafka-replay-log-producer.sh\n-rwxr-xr-x. 1 dyx dyx  874 11月 11 2017 kafka-replica-verification.sh\n-rwxr-xr-x. 1 dyx dyx 7027 11月 11 2017 kafka-run-class.sh\n-rwxr-xr-x. 1 dyx dyx 1376 11月 11 2017 kafka-server-start.sh\n-rwxr-xr-x. 1 dyx dyx  975 11月 11 2017 kafka-server-stop.sh\n-rwxr-xr-x. 1 dyx dyx  870 11月 11 2017 kafka-simple-consumer-shell.sh\n-rwxr-xr-x. 1 dyx dyx  945 11月 11 2017 kafka-streams-application-reset.sh\n-rwxr-xr-x. 1 dyx dyx  863 11月 11 2017 kafka-topics.sh\n-rwxr-xr-x. 1 dyx dyx  958 11月 11 2017 kafka-verifiable-consumer.sh\n-rwxr-xr-x. 1 dyx dyx  958 11月 11 2017 kafka-verifiable-producer.sh\ndrwxr-xr-x. 2 dyx dyx 4096 11月 11 2017 windows\n-rwxr-xr-x. 1 dyx dyx  867 11月 11 2017 zookeeper-security-migration.sh\n-rwxr-xr-x. 1 dyx dyx 1393 11月 11 2017 zookeeper-server-start.sh\n-rwxr-xr-x. 1 dyx dyx  978 11月 11 2017 zookeeper-server-stop.sh\n-rwxr-xr-x. 1 dyx dyx  968 11月 11 2017 zookeeper-shell.sh\n")])])]),e("ul",[e("li",[a._v("kafka-server-start.sh：启动kafka服务端的脚本，启动时需要指定server.properties配置文件")]),a._v(" "),e("li",[a._v("kafka-server-stop.sh：停止kafka服务端的脚本")]),a._v(" "),e("li",[a._v("kafka-topics.sh：对topic进行管理的脚本")])])]),a._v(" "),e("li",[e("p",[a._v("启动Kafka服务端（在三台机器上都启动kafka）：\n注意：启动kafka之前需要先启动zookeeper，另外在命令中可以添加"),e("code",[a._v("-daemon")]),a._v("选项可以使kafka在后台运行。")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-server-start.sh "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),a._v("/config/server.properties\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("2019-05-16 03:22:45,067"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v(" INFO KafkaConfig values:\n     advertised.host.name "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" null\n     advertised.listeners "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" null\n     advertised.port "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" null\n     alter.config.policy.class.name "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" null\n     authorizer.class.name "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("\n     auto.create.topics.enable "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("true")]),a._v("\n     auto.leader.rebalance.enable "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[a._v("true")]),a._v("\n     background.threads "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" 10\n     broker.id "),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v(" 0\n     "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),a._v(".\n")])])])]),a._v(" "),e("li",[e("p",[a._v("停止Kafka：")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-server-stop.sh stop\n")])])])]),a._v(" "),e("li",[e("p",[a._v("创建topic")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-topics.sh --zookeeper hadoop02:2181 --create --replication-factor 3 --partitions 2 --topic testtopic1\nCreated topic "),e("span",{pre:!0,attrs:{class:"token string"}},[a._v('"testtopic1"')]),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v(".")]),a._v("\n")])])]),e("ul",[e("li",[a._v("--topic 定义topic名")]),a._v(" "),e("li",[a._v("--replication-factor  定义副本数，副本数要小于启动的broker数量，否则会提示Error while executing topic command : replication factor: 3 larger than available brokers: 1\n"),e("ol",[e("li",[a._v("控制消息保存在几个broker(服务器)上，一般情况下等于broker的个数。")]),a._v(" "),e("li",[a._v("如果没有在创建时显示指定或通过API向一个不存在的topic生产消息时会使用broker(server.properties)中的default.replication.factor配置的数量")])])]),a._v(" "),e("li",[a._v("--partitions  定义分区数\n"),e("ol",[e("li",[a._v("控制topic将分片成多少个log。可以显示指定，如果不指定则会使用broker(server.properties)中的num.partitions配置的数量")]),a._v(" "),e("li",[a._v("虽然增加分区数可以提供kafka集群的吞吐量、但是过多的分区数或者或是单台服务器上的分区数过多，会增加不可用及延迟的风险。因为多的分区数，意味着需要打开更多的文件句柄、增加点到点的延时、增加客户端的内存消耗。")]),a._v(" "),e("li",[a._v("分区数也限制了consumer的并行度，即限制了并行consumer消息的线程数不能大于分区数")]),a._v(" "),e("li",[a._v("分区数也限制了producer发送消息是指定的分区。如创建topic时分区设置为1，producer发送消息时通过自定义的分区方法指定分区为2或以上的数都会出错的；这种情况可以通过alter –partitions 来增加分区数。")])])])])]),a._v(" "),e("li",[e("p",[a._v("查看topic")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-topics.sh --zookeeper hadoop02:2181 --list\n__consumer_offsets\ntesttopic1\n")])])])]),a._v(" "),e("li",[e("p",[a._v("在kafka中数据是如何存储的呢？存储在logs目录下")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ "),e("span",{pre:!0,attrs:{class:"token function"}},[a._v("cd")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),a._v("/logs\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 logs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ll\n总用量 2152\n-rw-rw-r--. 1 dyx dyx       0 4月  22 07:21 cleaner-offset-checkpoint\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-1\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-10\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-13\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-16\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-19\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-22\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-25\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-28\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-31\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-34\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-37\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-4\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-40\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-43\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-46\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-49\ndrwxrwxr-x. 2 dyx dyx    4096 4月  22 07:50 __consumer_offsets-7\n-rw-rw-r--. 1 dyx dyx  157151 5月  16 03:43 controller.log\n-rw-rw-r--. 1 dyx dyx   51707 4月  22 07:56 controller.log.2019-04-22-07\n-rw-rw-r--. 1 dyx dyx   34404 4月  22 08:56 controller.log.2019-04-22-08\n-rw-rw-r--. 1 dyx dyx   86414 4月  22 09:14 controller.log.2019-04-22-09\ndrwxrwxr-x. 2 dyx dyx    4096 5月  16 03:31 firsttopic-0\n-rw-rw-r--. 1 dyx dyx       0 4月  22 07:21 kafka-authorizer.log\n-rw-rw-r--. 1 dyx dyx       0 4月  22 07:21 kafka-request.log\n-rw-rw-r--. 1 dyx dyx   15249 5月  16 03:41 kafkaServer-gc.log.0.current\n-rw-rw-r--. 1 dyx dyx    1272 5月  16 03:31 log-cleaner.log\n-rw-rw-r--. 1 dyx dyx     172 4月  22 07:21 log-cleaner.log.2019-04-22-07\n-rw-rw-r--. 1 dyx dyx     378 4月  22 09:14 log-cleaner.log.2019-04-22-09\n-rw-rw-r--. 1 dyx dyx       4 5月  16 03:43 log-start-offset-checkpoint\n-rw-rw-r--. 1 dyx dyx      54 4月  22 07:21 meta.properties\n-rw-rw-r--. 1 dyx dyx     469 5月  16 03:43 recovery-point-offset-checkpoint\n-rw-rw-r--. 1 dyx dyx     469 5月  16 03:43 replication-offset-checkpoint\n-rw-rw-r--. 1 dyx dyx  144481 5月  16 03:43 server.log\n-rw-rw-r--. 1 dyx dyx   56703 4月  22 07:51 server.log.2019-04-22-07\n-rw-rw-r--. 1 dyx dyx     966 4月  22 08:51 server.log.2019-04-22-08\n-rw-rw-r--. 1 dyx dyx   12317 4月  22 09:14 server.log.2019-04-22-09\n-rw-rw-r--. 1 dyx dyx 1217850 5月  16 03:43 state-change.log\n-rw-rw-r--. 1 dyx dyx   98044 4月  22 07:50 state-change.log.2019-04-22-07\n-rw-rw-r--. 1 dyx dyx  178705 4月  22 09:14 state-change.log.2019-04-22-09\ndrwxrwxr-x. 2 dyx dyx    4096 5月  16 03:36 testtopic-0\ndrwxrwxr-x. 2 dyx dyx    4096 5月  16 03:43 testtopic1-0\ndrwxrwxr-x. 2 dyx dyx    4096 5月  16 03:43 testtopic1-1\n")])])]),e("p",[a._v("有两个目录testtopic1-0、testtopic1-1，进入其中一个")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 logs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ "),e("span",{pre:!0,attrs:{class:"token function"}},[a._v("cd")]),a._v(" testtopic1-0\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 testtopic1-0"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ll\n总用量 0\n-rw-rw-r--. 1 dyx dyx 10485760 5月  16 03:43 00000000000000000000.index\n-rw-rw-r--. 1 dyx dyx        0 5月  16 03:43 00000000000000000000.log\n-rw-rw-r--. 1 dyx dyx 10485756 5月  16 03:43 00000000000000000000.timeindex\n-rw-rw-r--. 1 dyx dyx        0 5月  16 03:43 leader-epoch-checkpoint\n")])])])]),a._v(" "),e("li",[e("p",[a._v("发送消息")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-console-producer.sh --broker-list hadoop02:9092 --topic testtopic1\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("\n")])])]),e("p",[a._v('注意，运行此命令指定的是broker-list节点而非zookeeper节点，9092是kafka对外的端口，同时此命令会进入">"，输入生产的消息。')])]),a._v(" "),e("li",[e("p",[a._v("消费消息（另开一个窗口，运行下面的命令）")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-console-consumer.sh --zookeeper hadoop02:2181 --topic testtopic1\nUsing the ConsoleConsumer with old consumer is deprecated and will be removed "),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("in")]),a._v(" a future major release. Consider using the new consumer by passing "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("bootstrap-server"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v(" instead of "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("zookeeper"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v(".\n")])])]),e("p",[a._v("提示信息：使用bootstrap-server选项代替zookeeper选项，因为在Kafka的新版本中，（Kafka-0.11以后）将offset维护在kafka集群中的leader副本中，即上面的所见的__consumer_offsets topic，并且默认提供了kafka_consumer_groups.sh脚本供用户查看consumer信息，而不放在zookeeper中，避免了多一次的访问zookeeper，提高了效率。")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-console-consumer.sh --bootstrap-server hadoop02:9092 --topic testtopic1\n")])])])]),a._v(" "),e("li",[e("p",[a._v("在生产者端发送数据（输入hello world），在消费者端可以消费到数据")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-console-producer.sh --broker-list hadoop02:9092 --topic testtopic1\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("hello world\n"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v(">")]),a._v("\n")])])]),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-console-consumer.sh --zookeeper hadoop02:2181 --topic testtopic1\nUsing the ConsoleConsumer with old consumer is deprecated and will be removed "),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v("in")]),a._v(" a future major release. Consider using the new consumer by passing "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("bootstrap-server"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v(" instead of "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("zookeeper"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v(".\nhello world\n")])])])]),a._v(" "),e("li",[e("p",[a._v("如果消费者端断开后重连，再消费则会接着以前消费过的数据往后消费，如果要消费者端从头开始消费，则是这个命令（添加--from-beginning）")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-console-consumer.sh --zookeeper hadoop02:2181 --topic testtopic1 --from-beginning \n")])])])]),a._v(" "),e("li",[e("p",[a._v("查看topic的详情信息")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-topics.sh --zookeeper hadoop02:2181 --describe --topic testtopic1\n  Topic:testtopic1        PartitionCount:2        ReplicationFactor:3     Configs:\n     Topic: testtopic1       Partition: 0    Leader: 0       Replicas: 0,1,2 Isr: 0,1,2\n     Topic: testtopic1       Partition: 1    Leader: 1       Replicas: 1,2,0 Isr: 1,2,0\n")])])]),e("p",[a._v("有两个分区，三个副本，Isr可以用于选举")])]),a._v(" "),e("li",[e("p",[a._v("删除topic")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-topics.sh --delete --zookeeper hadoop02:2181 --topic testtopic1  \n")])])])])]),a._v(" "),e("h2",{attrs:{id:"kafka工作流程分析"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#kafka工作流程分析","aria-hidden":"true"}},[a._v("#")]),a._v(" Kafka工作流程分析")]),a._v(" "),e("h3",{attrs:{id:"写入方式"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#写入方式","aria-hidden":"true"}},[a._v("#")]),a._v(" 写入方式")]),a._v(" "),e("p",[a._v("producer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（开辟了一整块磁盘空间，顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）")]),a._v(" "),e("h3",{attrs:{id:"分区"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#分区","aria-hidden":"true"}},[a._v("#")]),a._v(" 分区")]),a._v(" "),e("p",[a._v("消息发送时都被发送到一个topic，其本质就是一个目录，而topic是由一些Partition Logs(分区日志)组成，其组织结构如图\n"),e("a",{attrs:{"data-fancybox":"",title:"",href:"./assets/partition.png"}},[e("img",{attrs:{src:r(229),alt:""}})]),a._v(" "),e("a",{attrs:{"data-fancybox":"",title:"",href:"./assets/partition1.png"}},[e("img",{attrs:{src:r(230),alt:""}})])]),a._v(" "),e("p",[a._v("每个Partition中的消息都是有序的，生产的消息被不断追加到Partition log上，其中的每一个消息都被赋予了一个唯一的offset值。")]),a._v(" "),e("ol",[e("li",[a._v("分区的原因\n"),e("ul",[e("li",[a._v("方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了;")]),a._v(" "),e("li",[a._v("可以提高并发，因为可以以Partition为单位读写了;")])])]),a._v(" "),e("li",[a._v("分区的原则\n"),e("ul",[e("li",[a._v("指定了patition，则直接使用；")]),a._v(" "),e("li",[a._v("未指定patition但指定key，通过对key的value进行hash出一个patition;")]),a._v(" "),e("li",[a._v("patition和key都未指定，使用轮询选出一个patition;")])])])]),a._v(" "),e("h3",{attrs:{id:"副本"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#副本","aria-hidden":"true"}},[a._v("#")]),a._v(" 副本")]),a._v(" "),e("p",[a._v("同一个partition可能会有多个replication（对应server.properties配置中的 default.replication.factor=N）。没有replication的情况下，一旦broker 宕机，其上所有 patition 的数据都不可被消费，同时producer也不能再将数据存于其上的patition。引入replication之后，同一个partition可能会有多个replication，而这时需要在这些replication之间选出一个leader，producer和consumer只与这个leader交互，其它replication作为follower从leader 中复制数据。")]),a._v(" "),e("h3",{attrs:{id:"broker保存消息"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#broker保存消息","aria-hidden":"true"}},[a._v("#")]),a._v(" Broker保存消息")]),a._v(" "),e("ol",[e("li",[e("p",[a._v("数据的存储方式\nKafka具有存储功能，默认保存数据时间为7天或者大小1G，也就是说kafka broker上的数据超7天或者1G，就会被清理掉。这些数据存放在broker服务器上，以log文件的形式存在。物理上把topic分成一个或多个patition（对应 server.properties 中的num.partitions=3配置），每个patition物理上对应一个文件夹（该文件夹存储该patition的所有消息和索引文件）")]),a._v(" "),e("ol",[e("li",[e("p",[a._v("先创建一个topic，指定分区数为3，副本数为2")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop03 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-topics.sh --create --zookeeper hadoop02:2181 --partitions 3 --replication-factor 2 --topic topic0821\nCreated topic "),e("span",{pre:!0,attrs:{class:"token string"}},[a._v('"topic0821"')]),e("span",{pre:!0,attrs:{class:"token keyword"}},[a._v(".")]),a._v("\n")])])])]),a._v(" "),e("li",[e("p",[a._v("查看topic的信息")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop04 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ./kafka-topics.sh --zookeeper hadoop02:2181 --describe --topic topic0821\nTopic:topic0821 PartitionCount:3        ReplicationFactor:2     Configs:\n  Topic: topic0821        Partition: 0    Leader: 0       Replicas: 0,2   Isr: 0,2\n  Topic: topic0821        Partition: 1    Leader: 1       Replicas: 1,0   Isr: 1,0\n  Topic: topic0821        Partition: 2    Leader: 2       Replicas: 2,1   Isr: 2,1\n")])])])]),a._v(" "),e("li",[e("p",[a._v("分别在三个broker上的logs目录下存在该topic的三个partition数据")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ "),e("span",{pre:!0,attrs:{class:"token function"}},[a._v("cd")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),a._v("/logs\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 logs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ll\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),a._v(".\ndrwxrwxr-x. 2 dyx dyx    4096 5月  16 06:10 topic0821-0\ndrwxrwxr-x. 2 dyx dyx    4096 5月  16 06:10 topic0821-1\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 logs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ "),e("span",{pre:!0,attrs:{class:"token function"}},[a._v("cd")]),a._v(" topic0821-1   \n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop02 topic0821-1"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ll\n总用量 0\n-rw-rw-r--. 1 dyx dyx 10485760 5月  16 06:10 00000000000000000000.index\n-rw-rw-r--. 1 dyx dyx        0 5月  16 06:10 00000000000000000000.log\n-rw-rw-r--. 1 dyx dyx 10485756 5月  16 06:10 00000000000000000000.timeindex\n-rw-rw-r--. 1 dyx dyx        0 5月  16 06:10 leader-epoch-checkpoint\n")])])]),e("p",[a._v("在logs/topic0821-1/00000000000000000000.log中的一长串“0”是这个日志文件的offset位置。当日志文件达到时间或者大小的上限时，就会生成下一个日志文件，命名的就是下一个offset位置了。另外两个文件，index文件存放的是topic的offset，timeindex是存放的是时间戳")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop03 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ "),e("span",{pre:!0,attrs:{class:"token function"}},[a._v("cd")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),a._v("/logs\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop03 logs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ll\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),a._v(".\ndrwxrwxr-x. 2 dyx dyx   4096 5月   1 16:55 topic0821-1\ndrwxrwxr-x. 2 dyx dyx   4096 5月   1 16:55 topic0821-2\n")])])]),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop04 bin"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ "),e("span",{pre:!0,attrs:{class:"token function"}},[a._v("cd")]),a._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),a._v("/logs\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("[")]),a._v("dyx@hadoop04 logs"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("]")]),a._v("$ ll\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("..")]),a._v(".\ndrwxrwxr-x. 2 dyx dyx  4096 5月   2 19:13 topic0821-0\n/drwxrwxr-x. 2 dyx dyx  4096 5月   2 19:13 topic0821-2\n")])])])])])]),a._v(" "),e("li",[e("p",[a._v("数据的存储策略")]),a._v(" "),e("div",{staticClass:"language-shell extra-class"},[e("pre",{pre:!0,attrs:{class:"language-shell"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 日志保存时间 (hours|minutes)，默认为7天（168小时）。超过这个时间会根据policy处理数据。bytes和minutes无论哪个先达到都会触发。")]),a._v("\nlog.retention.hours"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("168\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[a._v("# 日志数据存储的最大字节数。超过这个时间会根据policy处理数据。")]),a._v("\nlog.retention.bytes"),e("span",{pre:!0,attrs:{class:"token operator"}},[a._v("=")]),a._v("1073741824\n")])])]),e("p",[a._v("需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。")])]),a._v(" "),e("li",[e("p",[a._v("offset存储方式\nconsumer消费后的offset有两种存储方式，一种是存放在broker的日志目录中，\n另一种方式是存放在zookeeper中。两种存放方式和你使用kafka-console-consumer命令使用的选项有关。如果使用的是bootstrap-server，那么就存放在broker；如果使用的是–zookeeper那么就存放在zookeeper。broker存放offset是kafka从0.9版本开始，提供的新的消费方式。原因是zookeeper来存放，还是有许多弊端，不方便灵活控制，效率不高。")])])]),a._v(" "),e("h3",{attrs:{id:"zookeeper的存储结构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#zookeeper的存储结构","aria-hidden":"true"}},[a._v("#")]),a._v(" zookeeper的存储结构")]),a._v(" "),e("ol",[e("li",[a._v("在zookeeper中存储Kafka有关的信息：\n"),e("a",{attrs:{"data-fancybox":"",title:"",href:"./assets/kafka_zk.png"}},[e("img",{attrs:{src:r(231),alt:""}})])]),a._v(" "),e("li",[a._v("使用ZooInspector工具查看Zookeeper节点信息：\n"),e("a",{attrs:{"data-fancybox":"",title:"",href:"./assets/broker1.png"}},[e("img",{attrs:{src:r(232),alt:""}})])])]),a._v(" "),e("h3",{attrs:{id:"producer的写入流程"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#producer的写入流程","aria-hidden":"true"}},[a._v("#")]),a._v(" Producer的写入流程")]),a._v(" "),e("ul",[e("li",[a._v('producer 先从 zookeeper集群保存的"/brokers/.../state" 节点找到该 partition 的 leader')]),a._v(" "),e("li",[a._v("producer 将消息发送给该 leader")]),a._v(" "),e("li",[a._v("leader 将消息写入本地 log")]),a._v(" "),e("li",[a._v("followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK")]),a._v(" "),e("li",[a._v("leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK")])]),a._v(" "),e("p",[e("a",{attrs:{"data-fancybox":"",title:"",href:"./assets/kafka_write.jpg"}},[e("img",{attrs:{src:r(233),alt:""}})])]),a._v(" "),e("p",[a._v("其中follower的ACK确认机制有三种：一般情况下存在三种情况：")]),a._v(" "),e("ul",[e("li",[a._v("At most once 消息可能会丢，但绝不会重复传输")]),a._v(" "),e("li",[a._v("At least one 消息绝不会丢，但可能会重复传输")]),a._v(" "),e("li",[a._v("Exactly once 每条消息肯定会被传输一次且仅传输一次")])]),a._v(" "),e("p",[a._v("当 producer 向 broker 发送消息时，一旦这条消息被 commit，由于 replication 的存在，它就不会丢。但是如果 producer 发送数据给 broker 后，遇到网络问题而造成通信中断，那 Producer 就无法判断该条消息是否已经 commit。虽然 Kafka 无法确定网络故障期间发生了什么，但是 producer 可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了 Exactly once，但目前还并未实现。所以目前默认情况下一条消息从 producer 到 broker 是确保了 At least once，可通过设置 producer 异步发送实现At most once。")]),a._v(" "),e("h3",{attrs:{id:"kafka的ack机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#kafka的ack机制","aria-hidden":"true"}},[a._v("#")]),a._v(" Kafka的ACK机制")]),a._v(" "),e("p",[a._v("当producer向leader发送数据时，可以通过request.required.acks参数来设置数据可靠性的级别：")]),a._v(" "),e("div",{staticClass:"language-properties extra-class"},[e("pre",{pre:!0,attrs:{class:"language-properties"}},[e("code",[e("span",{pre:!0,attrs:{class:"token attr-name"}},[a._v("request.required.acks")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[a._v("=")]),e("span",{pre:!0,attrs:{class:"token attr-value"}},[a._v("0")]),a._v("\n")])])]),e("ul",[e("li",[a._v("0：这意味着producer无需等待来自broker的确认而继续发送下一批消息。这种情况下数据传输效率最高，但是数据可靠性确是最低的。(不要求副本存到数据)")]),a._v(" "),e("li",[a._v("1（默认）：这意味着producer在ISR中的leader已成功收到的数据并得到确认后发送下一条message。如果leader宕机了，则会丢失数据。(要求一个副本存到数据，这个副本就是leader副本)")]),a._v(" "),e("li",[a._v("-1(或者是all)：producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。(要求所有副本都存到数据)\n但是这样也不能保证数据不丢失，比如当ISR中只有leader时（前面ISR那一节讲到，\nISR中的成员由于某些情况会增加也会减少，最少就只剩一个leader），这样就变成了acks=1的情况。")])]),a._v(" "),e("p",[a._v("如果要提高数据的可靠性，在设置request.required.acks=-1的同时，也要min.insync.replicas这个参数\n(可以在broker或者topic层面进行设置)的配合，这样才能发挥最大的功效。min.insync.replicas这个参数设定\nISR中的最小副本数是多少，默认值为1，当且仅当request.required.acks参数设置为-1时，此参数才生效。\n如果ISR中的副本数少于min.insync.replicas配置的数量时，客户端会返回异常：\norg.apache.kafka.common.errors.NotEnoughReplicasExceptoin: Messages are rejected since there are fewer in-sync replicas than required")]),a._v(" "),e("h2",{attrs:{id:"kafka高级api"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#kafka高级api","aria-hidden":"true"}},[a._v("#")]),a._v(" Kafka高级API")]),a._v(" "),e("p",[a._v("Kafka提供了两套API：高级API和低级API")])])},[],!1,null,null,null);t.default=s.exports}}]);